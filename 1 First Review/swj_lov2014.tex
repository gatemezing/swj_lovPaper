% Journal:
%   Journal of Ambient Intelligence and Smart Environments (JAISE), IOS Press
%   Web Intelligence and Agent Systems: An International Journal (wias)
%   Semantic Web: Interoperability, Usability, Applicability (SW)
% Latex 2e
% Test file iosart2c.tex

%[seceqn,secfloat,secthm,crcready]

% options: wias, jaise, sw
\documentclass{iosart2c}

\usepackage[T1]{fontenc}
\usepackage{times}%
\usepackage{listings}
\usepackage{tabularx}
%\usepackage{algorithm}
\usepackage{pdflscape}
\usepackage{paralist}
\usepackage{url}
\usepackage{natbib}% for bibliography sorting/compressing
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{endnotes}
\usepackage{floatrow}
\usepackage{bbding}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{epsfig,color,subfigure}
%\usepackage{tikz}
%\usepackage{tikzscale}
\usepackage{pgf-pie}
\usepackage{pgfplots}
\usepackage{soul}
\pgfplotsset{compat=1.5}
%\usetikzlibrary{backgrounds}
%\usepackage[scaled=0.87]{helvet}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{colortbl}   % Provides \rowcolor
\usepackage{calc}       % For doing math with dimensions
\usepackage{setspace}
\usepgfplotslibrary{dateplot}
%\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{shapes,decorations,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

%\usepackage{bibentry}


\newcommand{\figfontsize}{\footnotesize}

%%%%%%%%%%% Put your definitions here
\newcommand{\TODO}[1]{\textcolor{red}{\textbf{[TODO:#1]}}}
\newcommand{\maria}[1]{\textcolor{blue}{\textbf{[MARIA TO:#1]}}}
\newcommand{\py}[1]{\textcolor{olive}{\textbf{[PIERRE-YVES TO:#1]}}}
\newcommand{\ghis}[1]{\textcolor{brown}{\textbf{[GHIS TO:#1]}}}

% Language Definitions for Turtle
\definecolor{MyLightGray}{RGB}{200, 200,200}
\lstdefinelanguage{turtle}
{
    columns=fullflexible,
    keywordstyle=\color{red},
    morekeywords={PREFIX,SELECT,DISTINCT,UNION,FILTER,ORDER,BY,REGEX,STR,STRSTARTS,GRAPH,isBlank},
    morecomment=[l]{\#},
    tabsize=4,
    frame=lines,
    numbers=left,
    numberfirstline=true,
    xleftmargin=2.5em,
    framexleftmargin=2.8em,
    stepnumber=1,    
    firstnumber=1,
    alsoletter={-?}, % allowed in names
    morecomment=[s][\color{blue}]{<}{>},
    commentstyle=\color{green!40!black},
    basicstyle=\scriptsize\ttfamily\color{black},
    %numberstyle=\color{black},
    morestring=[b][\color{black}]\",
    backgroundcolor=\color{background},    
    showstringspaces=false
}

%% language json
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\scriptsize\ttfamily,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    numbers=left,
    numberfirstline=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}
%%%%%%%%%%% End of definitions

\newcolumntype{d}[1]{D{.}{.}{#1}}


\firstpage{1} \lastpage{5} \volume{1} \pubyear{2014}


\begin{document}

\begin{frontmatter}                        % The preamble begins here.

%
%\pretitle{Pretitle}
\title{Linked Open Vocabularies (LOV): a gateway to reusable semantic vocabularies on the Web}
%\title{LOV: An Ontology-based search engine for the Web}
%\thanks{Footnote in title.}}

\runningtitle{LOV: a gateway to reusable semantic vocabularies on the Web}
%\subtitle{Subtitle}

\review{Name Surname, University, Country}{Name Surname, University, Country}{Name Surname, University, Country}


\author[A]{\fnms{Pierre-Yves} \snm{Vandenbussche}\thanks{Thanks to Am\'elie Gyrard, Thomas Francart, Th\'er\`{e}ze Rogez and Anthony McCauley for their help on the project.}},
\author[B]{\fnms{Ghislain A.} \snm{Atemezing}},
\author[C]{\fnms{Mar\'ia} \snm{Poveda-Villal\'on}}
and
\author[D]{\fnms{Bernard} \snm{Vatant}}
\runningauthor{Pierre-Yves V. et al.}
\address[A]{Fujitsu (Ireland) Limited, Swords, Co. Dublin, Ireland\\
E-mail: pierre-yves.vandenbussche@ie.fujitsu.com}
\address[B]{Multimedia Communication Department, EURECOM, Campus SophiaTech
450, route des Chappes, 06410 Biot, France\\
E-mail: auguste.atemezing@eurecom.fr}
\address[C]{Ontology Engineering Group (OEG), 
Universidad Polit\'ecnica de Madrid, Madrid, Spain\\
E-mail: mpoveda@fi.upm.es}
\address[D]{Mondeca, 35 boulevard de Strasbourg, 75010 Paris, France
\\
E-mail: bernard.vatant@mondeca.com}


\begin{abstract}
%\textcolor{red}{The abstract should be clear, descriptive, self-explanatory and no longer than 200 words. It should also be suitable for publication in abstracting services. Do not include references or formulae in the abstract.}
One of the major barriers to the deployment of Linked Data is the difficulty that data publishers have in determining which vocabularies to use to describe the semantics of data. This system report describes Linked Open Vocabularies (LOV), a high quality catalogue of reusable vocabularies for the description of data on the Web. The LOV initiative gathers and makes visible indicators that have not been previously harvested such as interconnection between vocabularies, version history, maintenance policy, along with past and current referent (individual or organization). The LOV goes beyond existing Semantic Web search engines and takes into consideration the value's property type, matched with a query, to improve terms scoring. By providing an extensive range of data access methods (SPARQL endpoint, API, data dump or UI), we try to facilitate the reuse of well-documented vocabularies in the linked data ecosystem. We conclude that the adoption in many applications and methods of LOV shows the benefits of such a set of vocabularies and related features to aid the design and publication of data on the Web.
\end{abstract}

\begin{keyword}
LOV\sep Linked Open Vocabularies\sep Ontology search\sep Linked Data\sep Vocabulary catalogue
%\sep keyword five
\end{keyword}

\end{frontmatter}


\section{Introduction}
The last two decades has seen the emergence of a ``Semantic Web'' opening the way to a new use of the Web enabling humans and computer systems to exchange data with unambiguous, shared meaning. This vision has been realised through standards by the World Wide Web Consortium (W3C) such as the Resource Description Framework (RDF), RDF-Schema or the Web Ontology Language. 

\cite{janowicz2014five}.

In order to achieve a semantic interoperability, it is necessary that the vocabularies used to describe the data semantics reach a consensus to become \emph{de facto} standards. This is far from being a reality.
- Being accessible and sustainable
- Quality and stability
- Interlinked 


Although there are now tens of billions of facts spanning hundreds of Linked Datasets on the Web, 

Started in March 2011, as part of the DataLift research project \cite{scharffe_2012} and hosted by the Open Knowledge Foundation, the Linked Open Vocabularies (LOV) initiative is now an innovative observatory of the semantic vocabularies\footnote{In this paper, ``semantic vocabulary'', ``vocabulary'' and ``ontology'' terms are used interchangeably. An explanation of their meaning is given in the following section.} ecosystem. It gathers and makes visible indicators not yet harvested before, such as interconnection between vocabularies, versioning history, maintenance policy along with past and current referent (individual or organization) if any. The number of vocabularies indexed by LOV is constantly growing (511 as of June 2015) thanks to a community effort. It is the only catalogue, to the best of our knowledge, that provides all types of search criteria: metadata search, ontology search, APIs, comprehensive dump file and a SPARQL endpoint access. According to the categories of ontology libraries defined by D'Aquin and Noy~\cite{AquinJoWS12}, LOV falls under the categories \textit{``curated ontology directory''}  and \textit{``application platform''}. 

The development of the LOV has highlighted a number of interesting research challenges: \textit{``What are the solutions for long-term vocabulary preservation on the Web?"} \cite{Baker2013HLT}. This is a particularly important problem in a distributed and uncontrolled environment where any individual can create and publish a vocabulary that can then be reused by external publishers. This creates a dependency on the original vocabulary availability as it retains the semantics of the data. \textit{``How to facilitate vocabulary search and reuse"} \cite{butt2014, poveda2012landscape}. To be used by a broader community, reuse and design of vocabularies have to be facilitated by intuitive tools and methods.  \textit{``How can we harmonise the various curated vocabulary catalogues on the Web to ease their adoption?"} \cite{wasabi13}. One of the barriers to Semantic Web adoption is the confusion related to understanding and finding an appropriate vocabulary in compliance with best practice.

This report is structured as follows: In the next section, we describe the LOV architecture along with some high level results that the system has collected. In section \ref{sec:dataPubOntoEngine}, we explain how the LOV is used to support the Data Publication and Ontology Engineering process. Subsequently, we provide an overview of some applications and research projects based and motivated by the LOV system (section \ref{sec:lovecosystem}). In section \ref{sec:related}, we report on related work and reach out conclusions in section \ref{sec:conclusion}.

\section{LOV state}
As of June 2015, the LOV database contains 511 vocabularies. Figure \ref{fig:evolLOV} depicts the evolution of the number of vocabularies inserted in the LOV dataset. The addition of new vocabularies to LOV has been fairly constant with two outstanding events: 
\begin{inparaenum}[1)] 
	\item the increase beginning of 2012 corresponds to the deployment of LOV version 2 making automatic most of the vocabulary analyses and
	\item a small decrease and plateau beginning of 2015 corresponding to the deployement of LOV version 3. At that time we were thinking about removing offline vocabularies but finally decided to keep them with a special flag making LOV the last semantic continuity for datasets referencing a dead vocabularies.
\end{inparaenum} 

\begin{figure}[htb]
   \input{PGFPlots/LOVSizeEvol.tex}
   \caption{\label{fig:evolLOV} Evolution of the number of vocabularies in LOV from March 2011 to June 2015.}
\end{figure}

\begin{figure}[htb]
\input{PGFPlots/LOVCreaEvol.tex}
\caption{\label{fig:creaevol} Vocabularies distribution by creation date.}
\end{figure}

\begin{figure}[htb]
\input{PGFPlots/LOVModifEvol.tex}
\caption{\label{fig:modifevol} Vocabularies distribution by last modification date.}
\end{figure}

Looking at the creation date of the vocabularies registered in LOV, illustrated in Figure \ref{fig:creaevol}, it informs on the adoption of Semantic Web technologies. The apex of this bell curve is happening in 2011. The figure put as well this in the perspective of the W3C recommendation effort of RDF, RDFS and OWL languages. A decrease of vocabulary creation does not mean a decrease in use of the technology but rather than the coverage of existing vocabularies now covers a large part of the semantic description need. This is as well confirmed by figure \ref{fig:modifevol} that presents the last modification date of LOV vocabularies demonstrating a living ecosystem where vocabularies are not static. The median number of versions per vocabulary is 2. Where 98.23\% of vocabularies makes use of RDFS in addition to RDF, only 45.99\% of them makes use of OWL. Datasets usage are added to 127 vocabularies, from which 98 vocabularies are used in at least 2 datasets and 29 of them are described in only one dataset. 
% The last sentence for dataset usage comes from the endpoint containing 494 vocabularies, thus 25.70\% of vocabs with dataset info; from which 77,16\% reused by at least 2 datasets. 

Overall, LOV dataset contains almost 50,000 RDF vocabulary classes and properties with a median number of 9 and 17 respectively per vocabulary. Table \ref{tab:elements} presents a breakdown of LOV dataset content by vocabulary element type. Out of 511 vocabularies, 66.14\% use explicitly the English language for labels/comments. Table \ref{tab:language} presents the number and percentage of the top five languages detected in the LOV. Figure \ref{fig:langdist} shows the distribution of vocabularies per number of languages explicitly used: 27.98\% vocabularies still does not provide any language information and only 14.68\% of vocabularies are multilingual. 45 Languages are represented in LOV. We will discuss in section \ref{sec:conclusion} the importance for publishers to provide multilingual vocabularies on the Web.

\begin{table}[h!tb]
\caption{LOV vocabulary elements statistics.}
\begin{tabular}{lrr}
\hline
\textbf{Type} & \textbf{Count} &  \textbf{Median per vocab} \\ \hline
Classes & 20,034 & 9 \\
Properties & 29,925 & 17 \\
Instances & 5,232 & 0 \\
Datatypes & 101 & 0 \\
\hline  
\end{tabular}
\label{tab:elements}
\end{table}

 \begin{table}[h!tb]
\caption{Top five languages and percentage detected in the LOV catalogue. Some vocabularies can make use of multiple languages.}
\begin{tabular}{lrr}
\hline
\textbf{Language} & \textbf{Nb Vocabs} & \textbf{\% Vocabs (out of 511)}  \\ \hline
English & 338 & 66.14\%      \\
French & 37 & 7.24\%      \\
Spanish & 25 & 4.89\%      \\
German & 19 & 3.72\%      \\
Italian & 18 & 3.52\%      \\
\hline  
\end{tabular}
\label{tab:language}
\end{table}

\begin{figure}[htb]
\input{PGFPlots/LOVLangDist.tex}
\caption{\label{fig:langdist} Distribution of the vocabularies per language count.}
\end{figure}

From January to June 2015, more than 1.4 million searches have been issued on LOV\footnote{This figures include API and UI searches as well as searches without keywords such as ``all agents that participated in vocabulary design and publication in the geo-location domain''.}. A breakdown of the search per element type is provided in table \ref{tab:searchcategory}. We can see that the new feature of agent search (used for instance to identify experts of a domain in semantic web vocabulary design and publication) is the most used. Searches that includes keywords (and not only filters) concerns essentially vocabulary terms. Table \ref{tab:toptermsearch} presents the top 10 terms searched over that time period. Although most of the searches are performed through the User Interface, an application ecosystem using LOV APIs is surfaced as shown in the figure \ref{fig:apivsui}. 

\begin{table*}[h!tb]
\caption{Type of elements searched from January to June 2015 by users in LOV logs in total and with keyword.}
\begin{tabular}{l|rr|rr}
\hline
\textbf{Element Type } & \textbf{Nb searches} & \textbf{\% searches} & \textbf{Nb searches} & \textbf{\% searches} \\
 &  & & with keyword & with keyword \\ \hline
Term & 205,682 & 14.19\% & 80,728 & 92.84\% \\
Vocabulary & 178,837 & 12.34\% & 5,918 & 6.81\% \\
Agent & 1,064,597 & 73.47\% & 306 & 0.35\% \\
\hline  
\end{tabular}
\label{tab:searchcategory}
\end{table*}

 \begin{table}[h!tb]
\caption{Top 10 terms searched from January to June 2015 by users in LOV logs.}
\begin{tabular}{lrr}
\hline
\textbf{Vocabulary Term} & \textbf{Nb searches} & \textbf{\% searches} \\ \hline
set & 7,092 & 8.79\% \\
domain & 2,518 & 3.12\% \\
some & 2,473 & 3.06\% \\
status & 1,486 & 1.84\% \\
iso 639 & 1,389 & 1.72\% \\
same & 1,285 & 1.59\% \\
state & 1,235 & 1.53\% \\
supports & 1,145 & 1.42\% \\
start & 887 & 1.1\% \\
space & 864 & 1.07\% \\
\hline  
\end{tabular}
\label{tab:toptermsearch}
\end{table}

\begin{figure}[tb]
	\ffigbox{
		\figfontsize
		\input{PGFPlots/apiVsui.tex}
	}{
	  \caption{\label{fig:apivsui} Evolution of the number of searches through UI and API methods from January and June 2015.}%
	}
 \end{figure}

Over the last four years, the Linked Open Vocabularies initiative has gathered a community of around 470 people interested in various domains among them: ontology engineering or data publication. LOV Google+ community\footnote{\url{https://plus.google.com/communities/108509791366293651606}} is now an important place to discuss, report and announce general facts related to vocabularies on the Web. LOV dataset itself referenced 389 resources of type persons and 111 of type organization participating in vocabulary design and/or publication.


\section{System Components and Features}
	Figure \ref{fig:arch} shows an overview of the LOV components architecture. It is composed of four main components: 
\begin{inparaenum}[1)] 
	\item \emph{Tracking and Analysis}. Responsible to check for any vocabulary version update and analysing vocabularies specific features.
	\item \emph{Curation}. Ensure the high quality of the LOV dataset through methods for the community to suggest or edit the LOV catalogue.
	\item \emph{Data Access}. Consists in providing access to the data through a large range of methods and protocols to facilitate the use of LOV dataset and
	\item \emph{LOV User Interfaces and Application Program Interfaces}.
\end{inparaenum} 
Each components provides a set of features detailed in the following subsections.

%The intended purpose of LOV is to promote and facilitate the reuse of well documented vocabularies in the linked data ecosystem. To meet that goal, the LOV performs the following three main activities: 
%\begin{inparaenum}[1)] 
%  \item collecting new vocabularies from the LOV Community;
%  \item tracking and analysis of the LOV vocabulary catalogue; and
%  \item giving access to the data using various indices and publication methods to ease data consumption including a search engine, data dumps, SPARQL endpoint and APIs.
%\end{inparaenum}
%To carry out these tasks, the LOV is based on a number of components depicted in figure \ref{fig:arch}, relying on existing standards and open technologies.

\begin{figure}[ht!b]
\includegraphics[trim={0cm 8cm 0cm 0cm},scale=.6]{lov_architecture.pdf}
\caption{Overview of the Linked Open Vocabularies Architecture.}
\label{fig:arch}
\end{figure}

\subsection{Tracking and Analysis}
	The \emph{Tracking and Analysis} component takes care of dereferencing\footnote{URI is looked up over HTTP to return content in a processable format such as XML/RDF, notation 3 or turtle.} LOV vocabularies, storing a version locally (in notation 3 format) and extracting relevant metadata. A vocabulary consists of a collection of terms (classes and properties) expressed in W3C RDF, RDFS, OWL languages. 

\subsubsection{Vocabulary Level Analysis}
At the Vocabulary level, the system extracts three types of information for each vocabulary version (figure \ref{fig:dcat}):
\begin{itemize}
\item Metadata associated to the vocabulary: this information is explicitly defined within the vocabulary to provide context, and useful data about the vocabulary. To be part of the LOV catalogue, a vocabulary must contain some minimal metadata information \cite{vandenbussche2011metadata}. Some high level vocabularies can be reused for that purpose, such as Dublin Core to describe authors, contributors, publishers or Creative Commons\footnote{\url{http://creativecommons.org/ns#}} for the description of a license.

\item Inlinks vocabularies, making explicit the \underline{links from} another vocabulary based on the semantic relation of their terms.

\item Outlinks vocabularies, making explicit the\\ \underline{links to} another vocabulary based on the semantic relation of their terms.
\end{itemize}

There are many ways two vocabularies can be interlinked. Let's consider two vocabularies $V1$ and $V2$ such as $V1$ contains a class $c1$ and a property $p1$ and $V2$ contains a class $c2$ and a property $p2$. Relationships between these two vocabularies can be of the following types (the lines numbers in brackets correspond to real examples presented in listing \ref{list:voaf}):
		\begin{description}
			\item [Metadata.] some terms from $V2$ are reused to provide metadata about $V1$ (lines 1 to 2).

			\item [Import.] some terms from $V2$ are reused in complement with $V1$ to capture the semantic of the data (lines 3 to 4).

			\item [Specialization.] $V1$ defines some subclasses or subproperties (or local restrictions) of $V2$ (lines 5 to 8).

			\item [Generalization.] $V1$ defines some superclasses or superproperties of $V2$ (lines 9 to 11).

			\item [Extension.] $V1$ extends the expressivity of $V2$ (lines 12 to 15).

			\item [Equivalence.] $V1$ declares some equivalent classes or properties with $V2$ (lines 16 to 20).

			\item [Disjunction.] $V1$ declares some disjunct classes with $V2$ (lines 21 to 23).
		\end{description}
Where \texttt{dul} namespace is \url{http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#} and ontopic is \url{http://www.ontologydesignpatterns.org/ont/dul/ontopic.owl#} 

\begin{lstlisting}[float=*,basicstyle=\tiny,label={list:voaf}, language=turtle, caption={Examples of Inter-vocabulary relationships.}]
# Metadata
<http://www.w3.org/2004/02/skos/core> dct:title "SKOS Vocabulary"@en
# Import - V1 imports V2
<http://purl.org/NET/c4dm/event.owl> owl:imports <http://www.w3.org/2006/time>
# Specialization - c1 is subclass of c2
<http://open.vocab.org/terms/Birth> rdfs:subClassOf <http://purl.org/NET/c4dm/event.owl#Event>
# Specialization - p1 is subproperty of p2  
<http://purl.org/spar/fabio/hasEmbargoDate> rdfs:subPropertyOf <http://purl.org/dc/terms/date>
# Generalization - c1 has for narrower match c2 
<http://semanticweb.cs.vu.nl/2009/11/sem/Place> skos:narrowMatch 
    <http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing>
# Extension - p1 is inverse of p2
<http://vivoweb.org/ontology/core#translatorOf> owl:inverseOf <http://purl.org/ontology/bibo/translator>
# Extension - p1 has for domain c2
<http://xmlns.com/foaf/0.1/based_near> rdfs:domain <http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing>
# Equivalence - p1 is equivalent to p2
<http://lsdis.cs.uga.edu/projects/semdis/opus#journal_name> owl:equivalentProperty 
    <http://purl.org/net/nknouf/ns/bibtex#hasJournal>
# Equivalence - c1 is equivalent to c2 
<http://www.loc.gov/mads/rdf/v1#Language> owl:equivalentClass <http://purl.org/dc/terms/LinguisticSystem>
# Disjunction - c1 is disjoint with c2
<http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#TimeInterval>owl:disjointWith 
    <http://www.ontologydesignpatterns.org/ont/dul/ontopic.owl#SubjectSpace>
\end{lstlisting}

These relationships, with the exception of \emph{Import} which is represented by \url{owl:imports}, are captured by the Vocabulary of a Friend\footnote{\url{http://lov.okfn.org/vocommons/voaf/}} (VOAF) created for LOV representation needs. Table \ref{tab:voaf} presents a breakdown of the occurrences of each relation in LOV.

\begin{table}[h!tb]
\caption{Inter-vocabularies relationship types and their number of occurrences in LOV.}
\begin{tabular}{lcc}
\hline
\textbf{Inter vocabulary relationship} & \textbf{Nb Relations} \\ \hline
voaf:metadataVoc & 2637 \\
voaf:specializes & 1269 \\
voaf:extends & 1031 \\
owl:imports & 373 \\
voaf:hasEquivalencesWith & 201 \\
voaf:generalizes & 57 \\
voaf:hasDisjunctionsWith & 16 \\
\hline  
\end{tabular}
\label{tab:voaf}
\end{table}

\begin{figure}[ht!b]
\includegraphics[trim={0cm 10cm 0cm 0cm},width=1\textwidth]{FlowCharts/DCAT.pdf}
\caption{Metadata type, vocabulary inlinks and outlinks of DCAT vocabulary.}
\label{fig:dcat}
\end{figure}

\subsubsection{Vocabulary Term Level Analysis}
At the Vocabulary Term level, the system extracts labels that will be used for full text search and language information.

\subsection{Curation}

The vocabulary collection is maintained by curators in charge of validating\footnote{Before a vocabulary is inserted, LOV curators contact the authors to make sure the vocabulary is published following the best practices and contains enough metadata}, inserting a vocabulary in the LOV ecosystem and assigning a detailed review.

	\subsubsection{Vocabulary Insertion} Compared to other vocabulary catalogues (cf. section \ref{sec:related}), LOV relies on a semi-automated process for vocabulary insertion illustrated in figure \ref{fig:insertWorkflow}. Whereas an automated process put the emphasis on the volume, in our process, we focus on the quality of each vocabulary and therefore the quality of the overall LOV ecosystem. Suggestions are coming from the community and from inter-vocabulary reference links. Our system provides a feature to suggest\footnote{\url{http://lov.okfn.org/dataset/lov/suggest/}} the insertion of a new vocabulary. This feature allows a user to check what information the LOV application can automatically detect and extract. LOV curators then check if the vocabulary falls in the scope of LOV and if it meets basic vocabulary quality to be reused:
\begin{enumerate}
 \item a vocabulary should be dereferenceable
 \item a vocabulary should be parsable without error (warning tolerated)
 \item all classes and properties in a vocabulary should have an \texttt{rdfs:label}
 \item a vocabulary should refer and reuse relevant existing ones
\end{enumerate}
If a suggested vocabulary meets these criteria, it is then inserted in the LOV catalogue. during this process, LOV curators keep the authors informed and help them to improve their vocabulary quality. As a result of our experience in vocabulary publication, we are now able to publish a handbook about Metadata recommendations for linked open data vocabularies to help in publishing well documented vocabularies~\cite{vandenbussche2011metadata}.

\begin{figure}[ht!b]
\includegraphics[width=1\textwidth]{FlowCharts/DiagramInsert.pdf}
\caption{Curation workflow for vocabulary insertion.}
\label{fig:insertWorkflow}
\end{figure}


	\subsubsection{Vocabulary Review}
When some metadata failed to be extracted automatically (such as creators of a vocabulary), LOV curators enhance the description available in the system. The documentation provided by the LOV application assists any user in the task of understanding the semantics of each vocabulary term and therefore of any data using it. For instance, information about the creator and publisher is a key indication for a vocabulary user in case help or clarification is required from the author, or to assess the stability of that artifact. About 55\% of vocabularies specify at least one creator, contributor or editor. We augmented this information using manually gathered information, leading to the inclusion of data about the creator in over 85\% of vocabularies in the LOV. The database stores every version of a vocabulary since its first issue. For each version, a user can access the file (particularly useful when the original online file is no longer available). An automatic script is in place to automatically check for vocabulary updates on a daily basis. To embrace the complexity of the vocabulary ecosystem and assess the impact of a modification, one needs to know in which vocabularies and datasets a particular vocabulary term is referenced. To the best of our knowledge, LOV provides for the first time such a vision. 

\begin{figure}[ht!b]
\includegraphics[width=1\textwidth]{FlowCharts/DiagramCuration.pdf}
\caption{Curation workflow for systematic vocabulary review.}
\label{fig:reviewWorkflow}
\end{figure}

\subsection{Data Access}

LOV system (code and data) is published under Creative Commons 4.0 license\footnote{\url{https://creativecommons.org/licenses/by/4.0/}} (CC BY 4.0). Four methods are offered for users and applications to access the LOV data:
		\begin{inparaenum}[1)] 
			\item query the LOV search engine to find the most relevant vocabulary terms, vocabularies or agents matching keywords;
			\item download data dumps of the LOV catalogue in RDF Notation 3 format or the LOV catalogue and the latest version of each vocabulary in RDF N-quads format;
			\item run SPARQL queries on the LOV SPARQL Endpoint; and
			\item use the LOV system Application Program Interface (API) which provides a full access to LOV data for software applications.
		\end{inparaenum}


\subsubsection{Search Engine}
In \cite{butt2014}, Butt \emph{et al.} compare eight different ranking methods grouped in two categories for querying vocabulary terms:
\begin{itemize}
	\item Content-based Ranking Models: tf-idf, BM25, Vector Space Model and Class Match Measure.
	\item Graph-based Ranking Models: PageRank, Density Measure, Semantic Similarity Measure and Betweenness Measure
\end{itemize}
Based on their findings, we defined a new ranking method adapting \emph{Term frequency inverse document frequency} (tf-idf) to the graph-structure of vocabularies. When compared to the other methods, tf-idf takes into account the relevance and importance of a resources to the query when assigning a weight to a particular vocabulary for a given query term. We reuse the augmented frequency variation of term frequency formula to prevent a bias towards longer vocabularies. Because of the inherit graph structure of vocabularies, tf-idf need to be tailored so that instead of using a word as the basic unit for measuring, we are considering a vocabulary term $t$ in a vocabulary $V$ as the measuring unit. The equation \ref{eq:tfidf} presents the adaptation of tf-idf to vocabularies.

\begin{table}[h!tb]
\begin{tabular}{|l|l|}
  \hline
  \textbf{Variable} & \textbf{Description} \\ \hline
  $\mathcal{V}$ & Set of Vocabularies \\ \hline
  $V$ & A vocabulary: $V \in \mathcal{V}$ \\ \hline
  $N$ & Number of vocabularies in $\mathcal{V}$ \\ \hline
  $t$ & A vocabulary term URI (class, property, \\
       &  instance or datatype): $t \in V, t \in URI$ \\ \hline
  $Q$ & Query string \\ \hline
  $q_i$ & Query term $i$ of $Q$ \\ \hline
  $\sigma_V$ & Set of matched URIs for $Q$ in $V$ \\ \hline
  $\sigma_V(q_i)$ & Set of matched URIs for $q_i$ in $V$ : \\
         & $\forall t_i \in \sigma_V$ , $t_i \in V ,  t_i$ matches $q_i$ \\ \hline
  $p$ & A term predicate: $p \in URI$ \\ \hline
  $\mathcal{D}$ & Set of Datasets \\ \hline
  $D$ & A Dataset: $D \in \mathcal{D}$ \\ \hline
  $M(t_i)$ & Number of Datasets: $D$ in $\mathcal{D}$, $t_i \in D$ \\ \hline
\end{tabular}
\end{table}

\begin{equation}\label{eq:tfidf}
\begin{split}
tf(t,V) =0.5+ \frac{0.5 * f(t,V)}{max\left\{f(t_i,V): t_i \in V\right\}} \\
idf(t,\mathcal{V}) =\log\frac{N}{|\left\{V \in \mathcal{V}: t \in V\right\}|}
\end{split}
\end{equation}
 

As highlighted in \cite{butt2014} and \cite{schaible2013lover}, the notion of popularity of a vocabulary term across the LOD datasets set $\mathcal{D}$ is significantly important. In equation \ref{eq:pop} we introduce a popularity measure, function of the normalisation of the frequency $f(t,\mathcal{D})$ of a term URI $t$ in the set of datasets $\mathcal{D}$ and the normalisation of the number of datasets in which a term URI appears $M(t): t \in \mathcal{D}$. By using maximum in the normalisation we put the emphasis on the most used terms, result of a consensus within the community. This measure will give a higher score to terms that are often used in datasets and across a large number of datasets.


\begin{equation}\label{eq:pop}
\begin{split}
pop(t,\mathcal{D}) = \frac{f(t,\mathcal{D})}{max\left\{f(t_i,\mathcal{D}): t_i \in \mathcal{D}\right\}} \\
* \frac{M(t)}{max\left\{M(t_i): t_i \in \mathcal{D}\right\}}
\end{split}
\end{equation}

When compared to RDF datasets, best practices about vocabularies publication makes their structure consensual and stable. It becomes then intuitive to assign more importance of a vocabulary term matching a query on the value of the property \url{rdfs:label} than \url{dcterms:comment}. The equation \ref{eq:norm} extends the lucene based search engine elasticsearch inner field-length norm $lengthNorm(field)$, which assigns a heigher weight to shorter fields, by combining it with a property-level boost $boost(p(t))$. Using this property-level boost we can assign a different score depending on which label property a query term matched. We distinguish four different label property categories on which a query term could match: 
		\begin{itemize}
 			\item Local name (URI without the namespace). While a URI is not suppose to carry any meaning, it is a convention to use a compressed form of a term label to construct the local name. It becomes therefore an important artifact for term matching for which the highest score will be assigned. An example of local name matching the term ``person'' is \url{http://schema.org/Person};
			\item Primary labels. The highest score will also be assigned for matches on \url{rdfs:label}, \url{dce:title}, \url{dcterms:title}, \url{skos:prefLabel} properties. An example of primary label matching the term ``person'' is \url{rdfs:label} \emph{"Person"@en};
			\item Secondary labels. We define as secondary label the following properties: \url{rdfs:comment}, \url{dce:description}, \url{dcterms:description}, \url{skos:altLabel}. A medium score is assigned for matches on these properties. An example of secondary label matching the term ``person'' is \url{dcterms:description} \emph{"Examples of a Creator include a person, an organization, or a service."@en}; and
			\item Tertiary labels. Finally all properties not falling in the previous categories are considered as tertiary labels for which a low score is assigned. An example of tertiary label matching the term ``person'' is \url{http://metadataregistry.org/uri/profile/RegAp/name} \emph{"Person"@en}. 
		\end{itemize}

\begin{equation}\label{eq:norm}
\begin{split}
norm(t,V) =  lengthNorm(field) \\
* \prod_{p \in V} boost(p(t))
\end{split}
\end{equation}

For every vocabulary in the LOV, terms (classes, properties, datatypes, instances) are indexed and a full text search feature is offered\footnote{\url{http://lov.okfn.org/dataset/lov/terms}}. Human users or agents can further narrow a search by filtering on term type (class, property, datatype, instance), language, vocabulary domain and vocabulary.

The final score of $t$ for a query $Q$ (equation \ref{eq:score}) is a combination of tf-idf, the importance of label properties of $t$ on which query terms matched and the popularity of that term in LOD dataset. While the factorisation of tf-idf and field normalisation factor is common for search engine ranking\footnote{See elasticsearch documentation: \url{http://bit.ly/1e37sFL}}, we add a fourth parameter - the popularity - as it is fundamental in the Semantic Web. Indeed, the intention of LOV is to foster the reuse of consensual vocabularies that become de facto standards. The popularity metric provides an indication on how widely a term is already used (in frequency and in number of datasets using it). We therefore add this new factor specific to Semantic Web to the scoring equation.
 

\begin{equation}\label{eq:score}
\begin{split}
Score(t,Q) =tf(t,V) * idf(t,\mathcal{V}) \\
* norm(t,V) * pop(t,\mathcal{D})\\
: \forall t\left\{\exists q_i \in Q: t \in \sigma_V(q_i)\right\}
\end{split}
\end{equation}





\subsubsection{Data Dumps}
The system provides data dumps of the LOV vocabulary catalogue in RDF Notation 3 format\footnote{\url{http://lov.okfn.org/lov.n3.gz}} and the LOV catalogue along with the latest version of each vocabulary in RDF N-quads format\footnote{\url{http://lov.okfn.org/lov.nq.gz}}. As illustrated in figure \ref{fig:model}, the RDF model mainly reuses the Data CATalogue Vocabulary (DCAT) which allows the representation of the LOV catalogue as a \texttt{dcat:Catalog} composed of vocabulary entries (\texttt{dcat:CatalogRecord}) capturing information like the insertion date in LOV. Each entry point to the vocabulary itself is represented by a sub class of \texttt{dcat:Dataset} defined in the Vocabulary Of A Friend (VOAF). This artifact contains metadata extracted by LOV application such as creators, first issued date, number of occurrences of the vocabulary in Linked Open Data. Each vocabulary is then linked to its various published versions represented by the \texttt{dcat:Distribution} entity on which information such as inter vocabulary links or languages can be found.

\begin{figure*}[!htb]
\includegraphics[scale=0.5]{model.png}
\caption{UML class diagram representation of LOV catalogue RDF schema model.}
\label{fig:model}
\end{figure*}


\subsubsection{SPARQL Endpoint}
The LOV SPARQL Endpoint\footnote{\url{http://lov.okfn.org/dataset/lov/sparql}} offers a complementary data access method and allows clients to pose complex queries to the server and retrieve direct answers computed over the LOV dataset. We use Jena fuseki triple store to store the N-quads file containing the LOV catalogue and the latest version of each vocabulary. This allows for the first time to query multiple vocabulary at the same time and to detect inter-vocabulary dependencies. An example of this use is explained in section \ref{sec:dataPubOntoEngine}.

\subsection{LOV User Interfaces and Application Program Interfaces}
LOV APIs give a remote access to the many functions of LOV through a set of RESTful services\footnote{\url{http://lov.okfn.org/dataset/lov/apidoc/}}. The basic design requirements for these APIs is that they should allow applications to get access to the very same information humans do via the User Interfaces. More precisely the APIs give access, through three different type of services (cf. figure \ref{fig:apis}), to functions related to:
\begin{itemize} 
			\item Vocabulary terms (classes, properties, datatypes and instances). With these functions, a software application can query the LOV search engine, ask for autocompletion or suggestion for misspelled terms;
			\item Vocabularies. A client can get access to the current list of vocabularies contained in the LOV catalogue; search for vocabularies, get autocompletion or obtain all details about a vocabulary; and
			\item Agents. This provides a software agent with a list of all agents references in the LOV catalogue, a mean to search for an agent, get autocompletion and details about an agent.
		\end{itemize}
LOV APIs is a convenient manner to get access to the full functionality and data of the LOV. It is particularly appropriate for dynamic Web applications using scripting languages such as Javascript. The APIs described above have been developed for, and following the requirements of, Ontology Design and Data Publication tools.

\begin{figure}[ht!b]
\includegraphics[scale=0.4]{apis.png}
\caption{List of APIs to access LOV data.}
\label{fig:apis}
\end{figure}

The LOV Website offers an intuitive navigation within the vocabularies catalogue. It allows users to explore vocabularies, vocabulary terms, agents, languages and to get the connection between these entities. For instance, a user can look for experts in \emph{geography} and \emph{geometry} domains\footnote{\url{http://lov.okfn.org/dataset/lov/agents?&tag=Geography,Geometry}}. We use d3\footnote{\url{http://d3js.org/}} javascript library to display charts and make the navigation more interactive like star graph representation used to display incoming and outgoing links between vocabularies (cf. figure \ref{fig:graphVocab}). 


\begin{figure}[ht!b]
\includegraphics[scale=0.45]{graphVocab.png}
\caption{Schema.org vocabulary incoming and outgoing links graphical representation as displayed in the UI.}
\label{fig:graphVocab}
\end{figure}




\section{LOV as a support for Data Publication and Ontology Engineering}
\label{sec:dataPubOntoEngine}


LOV can be used in any methodology for the creation and reuse of ontologies. One of the most mature methodology for supporting ontology development is NeOn.  
The NeOn Methodology is a scenario-based methodology that supports the collaborative aspects of ontology development and reuse, as well as the dynamic evolution of ontology networks in distributed environments \cite{MC10}. 
%The key assets of the NeOn Methodology are \cite{MC10}:
%\begin{itemize}
% \item  A set of nine scenarios for building ontologies and ontology networks, emphasizing the reuse of ontological and non-ontological %resources, the re-engineering and merging, and taking into account collaboration and dynamism.
% \item The NeOn Glossary of Processes and Activities, which identifies and defines the processes and activities carried out when ontology %networks are collaboratively built by teams.
% \item Methodological guidelines for different processes and activities of the ontology network development process, such as the reuse and re-%engineering of ontological and non-ontological resources, the ontology requirements specification, the ontology localization, the scheduling, %etc.
%\end{itemize}

Based on the NeOn Methodology's glossary of activities for building ontologies, the LOV system is relevant in four activities:

\begin{description}

 \item [Ontology Search.] Main LOV's feature is the search of vocabulary terms. These vocabularies are categorized within the LOV according to the domain they address. In this way, the LOV system contributes to ontology search by means of (a) keyword search and (b) domain browsing.
 \item [Ontology Assessment.] LOV provides a score for each term retrieved by a keyword search. This score can be used during the assessment stage.
 \item [Ontology Mapping.] In LOV, vocabularies rely on each other in seven different ways. These relationships are explicitly stated using VOAF vocabulary. This data could be useful to find alignments between ontologies, for example one user might be interested in finding equivalent classes for a given class or all equivalent classes and equivalent properties among two ontologies. Listing \ref{list:alignment} shows the SPARQL query to retrieve all the equivalent classes between the vocabularies \texttt{foaf} and \texttt{schema}\footnote{The reader can run the query on LOV Endpoint: \url{http://bit.ly/1Lqybcu}.}. Table \ref{tab:eqCR} shows the alignments between foaf and schema vocabularies.
     
 \begin{lstlisting}[basicstyle=\tiny,float=htb,caption={SPARQL query asking for all the equivalent classes and properties between the vocabularies foaf and schema.},label=list:alignment, language=turtle]
SELECT DISTINCT ?elem1 ?alignment ?elem2{
 GRAPH ?g{
  ?elem1 rdfs:isDefinedBy 
    <http://xmlns.com/foaf/0.1/>.
  {?elem1 owl:equivalentClass ?elem2}
  UNION {?elem2 owl:equivalentClass ?elem1}
  FILTER(STRSTARTS(STR(?elem2), 
     "http://schema.org/"))
  ?elem1 ?alignment ?elem2.
}}
	
	\end{lstlisting}
	


 \begin{table}[h!tb]
\caption{Equivalent classes and properties between foaf and schema.}
\scalebox{0.81}{
\begin{tabular}{lll}
\hline
\textbf{elem1} & \textbf{alignment} & \textbf{elem2} \\ \hline
foaf:Document & owl:equivalentClass & schema:CreativeWork \\
foaf:Image & owl:equivalentClass & schema:ImageObject \\
foaf:Person & owl:equivalentClass & schema:Person \\
\hline  
\end{tabular}
}
\label{tab:eqCR}
\end{table}
    
% \item [Ontology Localization.] Labels in different languages are stored in the LOV endpoint. This annotations could be used when translating terms into different languages. This information could be extracted by querying the SPARQL endpoint\footnote{Result of the query can be found at the following URL: \url{http://goo.gl/JJCJ01}} as shown in Listing \ref{list:person} where all the labels defined for the terms that have at least one \url{rdfs:label} containing strictly ``person":
		
%    \begin{lstlisting}[basicstyle=\tiny,float=htb,caption={SPARQL query asking all the labels defined for the terms containing person.},label=list:person, language=turtle]
% SELECT DISTINCT ?label2 ?element{
%   ?element rdfs:label ?label1 .
%   ?element rdfs:label ?label2 .
%   FILTER (?label1 != ?label2 ).
%   FILTER(REGEX(STR(?label1), "person", "i")).
% } ORDER BY ?element
%	\end{lstlisting}
%							
%   An excerpt of the query result is shown in Figure \ref{fig:translations}. From that result, ``Persona''@es and ``Personne''@fr could be used as translations for the English term ``Person'' in Spanish and French respectively. 
%   
%   \begin{figure}[ht!b]
%     \centering
%     \includegraphics[width=.90\linewidth]{translations1.png}
%     \caption{Translations example for foaf:Person}
%     \label{fig:translations}
%   \end{figure}
   
\end{description}

%Figure \ref{fig:LOVandNeOn} shows the activities within the overall NeOn methodologies activity workflow that can benefit from the LOV.
%\begin{figure}[h!tp]
%\centering
%  \includegraphics[width=1\linewidth]{neonScenarios.png}
%  \caption{Meeting points between the LOV and the NeOn methodology, derived from \cite{MC10}.}
%  \label{fig:LOVandNeOn}
%\end{figure}

\section{LOV Adoption}
\label{sec:lovecosystem}
LOV is supporting the emergence of a rich application ecosystem thanks to its various data access methods. We list below some tools using our system as part of their service and projects.
 
\subsection{Derived tools and applications}

Maguire et al. \cite{ontomaton12} uses LOV search API to implement OntoMaton\footnote{\url{https://github.com/ISA-tools/OntoMaton}}, a widget for bringing together ontology lookup and tagging within the collaborative environment provided by Google spreadsheets. 

YASGUI (Yet Another SPARQL Query GUI)\footnote{\url{http://legacy.yasgui.org/}} is a client-side JavaScript SPARQL query editor that uses the LOV API for property and class autocompletion together with \url{http://prefix.cc} for namespace prefix autocompletion \cite{yasgui}.

Datalift\footnote{\url{http://datalift.org/}} platform \cite{scharffe_2012}, a framework for ``lifting'' raw data into RDF, comes with a module to map data objects and properties to ontology classes and predicates available in the LOV catalogue. Data2Ontology module takes as input a ``raw RDF'', a dataset that has been converted directly from legacy format to triples. The goal is to help publishers reuse existing ontologies for converting their dataset owing easy discovery and interlinking. 
%It consists of three main components assisting the publisher in selecting properties suitable for the dataset to be published. 
%\begin{description}
%\item 1-LOV component. This component connects with the LOV catalogue to retrieve up-to-date ontologies using the LOV %search API\footnote{\url{http://lov.okfn.org/dataset/lov/apidoc/#lov2search}}.
%\item 2-Matching Workflow. Data2Ontology maps the data to LOV by automatically proposing a list of best matches.
%\item 3-SPARQL Generator. This module receives as input the desired mappings and creates the SPARQL CONSTRUCT query needed to %implement the mapping. The query can further be modified before the execution to generate a new dataset in the lifting process with Datalift.
%\end{description}

OntoWiki\footnote{\url{http://ontowiki.net/}} facilitates the visual presentation of a knowledge base as an information map, with different views on instance data \cite{auer2006ontowiki}. It enables intuitive authoring of semantic content, with an inline editing mode for editing RDF content, similar to WYSIWIG for text documents. OntoWiki offers a vocabulary selection feature based on LOV.

Furthermore, we can mention the Prot{\'e}g{\'e}LOV\footnote{\url{http://labs.mondeca.com/protolov/}} tool which provides a technological support to help users in reusing terms based on the vocabularies in the LOV catalogue, mostly by retrieving the classes, object properties and data properties according to the panel in the editor during the modeling. It is a plug-in for Prot{\'e}g{\'e} editor tool \cite{prolov2015} for improving the development of lightweight ontologies by reusing existing vocabularies at low fine grained level. The tool search for a term in LOV via APIs and provides three actions if the term exists : (i) replace the selected term in the current ontology, (ii) add the \texttt{rdfs:subClassOf} axiom and (iv) add the \texttt{owl:equivalentClass}.


\subsection{Using LOV as a Research platform}

LOV has served as the object of studies in \cite{poveda2012landscape} where Poveda-Villal\'on \emph{et al.} analysed trends in ontology reuse methods. In addition, the LOV dataset has been used in order to analyze the occurrence of good and bad practices in vocabularies \cite{poveda2013detecting}.

Prefixes in the LOV dataset are regularly mapped with namespaces in the prefix.cc service. In \cite{wasabi13}, the authors perform alignments of Qnames of vocabularies in both services, and provide different solutions to handle clashes and disagreements between preferred namespaces. Both  LOV and prefix.cc provide associations between prefixes and namespaces but follow a different logic. The prefix.cc service supports polysemy and synonymy, and has a very loose control on its crowd-sourced information. In contrast, LOV has a much more strict policy forbidding polysemy and synonymy ensuring that each vocabulary in the LOV database is uniquely identified by a unique prefix identification allowing the usage of prefixes in various LOV publication URIs. This requirement leads sometimes to a situation where the LOV use prefixes differently from the ones recommended by the vocabulary publishers.

The LOV query log covering the period between 2012-01-06 and 2014-04-16 is used in \cite{butt2014} to build a benchmark suite for ontology search and ranking. The CBRBench\footnote{\url{https://zenodo.org/record/11121}} benchmark uses eight ranking models of resources in ontologies and compares the results with ontology engineers. We plan to start a collaboration with the authors to enhance the LOV search based on the study result.

In \cite{janowicz2014five}, the authors rate vocabularies according to some criteria beyond the \texttt{sameAs} links such as \texttt{subClassOf} and \texttt{equivalentClass} `links' between vocabularies to foster interoperability, query federation and better interpretation of data. %\ghis{ read this paper to add more content here}

RDFUnit\footnote{\url{https://github.com/AKSW/RDFUnit}} is a test-driven data debugging framework for the Web of Data. In \cite{rdfunit}, the authors provide an automatic test case for all available schema registered with the LOV. Vocabularies are used to encode semantics to domain specific knowledge to check the quality of data.

Governatori et al. \cite{governatori2014} analyzes the current use of licenses in vocabularies on the Web based on the LOV catalogue to further propose a framework to detect incompatibilities between datasets and vocabularies.


\section{Related work}
\label{sec:related}
Reusing vocabularies requires searching for terms in existing specialized vocabulary catalogues or search engines on the web. While we refer the reader to~\cite{AquinJoWS12} for a systematic survey of ontology repositories, we list below some existing catalogues relevant to find vocabularies:  

\begin{itemize}
 \item \textit{Catalogues of generic vocabularies/schemas} similar to LOV catalogue. Example of catalogues falling in this category are vocab.org\footnote{\url{http://vocab.org/}}, ontologi.es\footnote{\url{http://ontologi.es/}}, JoinUp Semantic Assets or the Open Metadata Registry.
 \item \textit{Catalogues of ontologies for a specific domain} such as biomedicine with the BioPortal \cite{bioportal11}, geospatial ontologies with SOCoP+OOR\footnote{\url{https://ontohub.org/socop}}, Marine Metadata Interoperability and the SWEET \cite{sweet05} ontologies\footnote{\url{http://sweet.jpl.nasa.gov//}}. The SWEET ontologies include several thousand terms, spanning a broad extent of Earth system science and related concepts (such as data characteristics), with the search tool to aid finding science data resources. 
 \item \textit{Catalogues of ontology Design Patterns (ODP)} focused on reusable patterns in ontology engineering \cite{presutti08}. The submitted patterns are small pieces of vocabularies that can further be integrated or linked with other vocabularies. ODP does not provide a search function for specific terms as is the case with Swoogle or Watson.
 \item \textit{Search Engines of ontology terms}. Among ontology search engines, we can cite: Swoogle \cite{finin2005swoogle}, Watson \cite{d2007watson,Sabou07} and FalconS \cite{cheng2008falcons}. These search engines crawl for data schema from RDF documents on the Web. They offer a filtering based on ontology type (Class, Property) and a ranking based on the popularity. They don't look for ontology relations nor check if the definition of the ontology is available (usually known as dereferenciation)
\end{itemize}

LODStats \cite{demter-2012-ekaw} is another project based on a statement-stream-based approach for gathering comprehensive statistics for RDF resources in datahub. LODStats gathers up to 32 different statistical criteria and use the Vocabulary of Interlinked Datasets (VoID)\cite{void2009} and the Data Cube vocabulary to represent these statistics. LODStats maintains a comprehensive statistics on the schema elements (i.e. classes, properties) defined and used in a dataset. LOV is similar to the approach presented by Demter et al. as it provides statistics at the schema level dimension. Another difference between LODStats and LOV is that our work do not only depends on automatic crawl of the CKAN dataset, but also on suggestions made by users with human curators on the loop for quality checking and review of the vocabulary. For example, while LODStats reports 2825 vocabularies, there are many duplicates representing in fact the same vocabulary URI (e.g., foaf has three different records\footnote{\url{http://stats.lod2.eu/vocabularies?search=foaf}}, while this is not the case in LOV. %LODStats result for datasets usage  pull in LOV to represent the occurrences of datasets used in the LOD cloud. 

A updated comprehensive empirical survey of Linked Data conformance is presented by Schmachtenberg et al. \cite{max2014}. Their survey is based on a large-scale Linked Data crawl from March 2014 as well as detailed categorize datasets by topical domain to analyze the differences in the adoption of the best practices in different domains. Their results concerning the most used accessible vocabularies vocabularies (e.g., foaf, dcterms, skos, etc.) and the adoption of well-known vocabularies are inline with the findings of this paper. The general difference of our work with the one presented by Schmachtenberg et al. is that our approach is vocabulary-based while their approach is dataset-based. On the other hand, deferenceability of a vocabulary is a strong requirement of a vocabulary to be inserted into LOV, which is not always the case in their work (e.g., they define the notion of partly dereferencable for vocabularies).




\section{Discussion}

LOV main strength focuses mainly on giving access to up-to-date of highly curated vocabularies (subpart of semantic documents of the Web) submitted by the community, reviewed and validated by curators. In addition, LOV keeps track locally of all versions of the vocabularies.  In contrast, Swoogle is designed to automatically discover Semantic Web Documents (SWDs), index their metadata. Thus, the result of a search query retrieved any semantic document. For example, a query of the term \textit{person} gives $16,438$ results while in LOV, the term only appears in $1,562$ vocabularies.
Watson works similarly to Swoogle, crawling and indexing semantic documents at a small scale, explicitly distinguishing for each document (resource), concepts, properties and individuals if available. While in Swoogle  the ranking score is displayed, Watson shows the language of the resource and the size. Falcons is a keyword-based search system for concepts and objects on the Semantic Web, and is equipped with entity summarization for browsing. It is notable that Falcons limits the search only to ontologies and a recommendation feature is provided according to users' preferences. However, it does not provide any relationships between the related ontologies, nor any domain classification of the vocabularies.
Table \ref{tab:lovfeatures} presents a summary of key features of LOV with respect to Swoogle, Watson and Falcons.
 \begin{table*}[!htb]
\centering{
\begin{tabular}{lllll}
\hline
 \textbf{Feature}	& Swoogle & Watson & Falcons & LOV 			 \\ \hline
Browsing ontologies	   & Yes & Yes & Yes & Yes \\
Ontology siscovery method   & Automatic & Automatic & Automatic & Manual \\
Scope & SWDs & SWDs & Concepts & Ontologies \\
Ranking	& LOD popularity & LOD popularity & LOD popularity &  LOD/LOV popularity\\
&&&&+ property semantic score	 \\
Domain filtering & No & No & No & Yes \\
Comments and review 	& No & Yes & No & Only by curators	\\
Web service access & Yes & Yes & Yes & Yes		\\
SPARQL endpoint	& No & No & No & Yes		\\
Read/Write	& Read & Read \& Write & Read &Read  	\\
Ontology directory & No & No & No &Yes \\
Application platform & No & No & No & Yes \\
Storage & Cache & - & - & Dump \& endpoint \\
Interaction with contributors & No &  - & No & Yes \\

		\\ \hline

\end{tabular}
\caption{Comparison of LOV with respect to Swoogle, Watson and Falcons; adapted from the framework presented by d'Aquin and Noy \cite{AquinJoWS12}.  }
\label{tab:lovfeatures}
}
\end{table*}

Apart from the core features for ontology development as search, ranking and reuse, LOV provides to users with useful functionalities and metadata information  for ontology development processes and activities. his makes it possible to find the most suitable classes and properties to express data as Linked Data. For example, one could take advantage of LOV to localize ontologies or provide multilingual annotation as \texttt{RDFS} labels in many different languages are stored in LOV endpoint. This annotations can be used when translating terms into different languages or to provide multilingual search based on ontologies. 

However, LOV itself can not replace the creation of term in an engineering process. Thus, LOV can be used as input for a vocabulary recommendation system such as the LOVER approach presented by Schaible et al.\cite{schaible2013lover}. Furthermore, LOV does not currently support reasoning capabilities (such as transitive closure or schema matching techniques) on top of the vocabularies. Although such information would potentially be relevant to the LOV users, this  out of scope of the current effort of explicitly represent the links between vocabularies using VOAF and maintaining version history.


		
%    \begin{lstlisting}[basicstyle=\tiny,float=htb,caption={SPARQL query asking all the labels defined for the terms containing person.},label=list:person, language=turtle]
% SELECT DISTINCT ?label2 ?element{
%   ?element rdfs:label ?label1 .
%   ?element rdfs:label ?label2 .
%   FILTER (?label1 != ?label2 ).
%   FILTER(REGEX(STR(?label1), "person", "i")).
% } ORDER BY ?element
%	\end{lstlisting}
%							
%   An excerpt of the query result is shown in Figure \ref{fig:translations}. From that result, ``Persona''@es and ``Personne''@fr could be used as translations for the English term ``Person'' in Spanish and French respectively. 
%   
%   \begin{figure}[ht!b]
%     \centering
%     \includegraphics[width=.90\linewidth]{translations1.png}
%     \caption{Translations example for foaf:Person}
%     \label{fig:translations}
%   \end{figure}

%The LOV search engine is to the best of our knowledge, the only purpose-built ontology search engine available on the Web with an up-to-date index.

\section{Conclusion and Future work}
\label{sec:conclusion}
In this system report, we presented an overview of the Linked Open Vocabularies initiative. The importance of this work is motivated by the difficulty for data publishers to determine which vocabularies to use to describe their data. The key innovations described in this article include: 
\begin{inparaenum}[1)] 
	\item the availability of a high quality vocabularies dataset through multiple accessing methods;
	\item the vocabulary metadata curation by experts, making explicit for the first time the relationships between vocabularies and their version history; and
	\item the consideration of property semantic in term search scoring.
\end{inparaenum}

The adoption and integration of LOV catalogue in applications for vocabulary engineering, reuse and data quality are significant. Linked Open Vocabularies have a central role in vocabulary life-cycle on the Web of Data as highlighted by the W3C\footnote{\url{http://www.w3.org/2013/data/}}. In the future, we see in particular the following directions for advancing LOV initiative:

\emph{A comprehensive list of most used vocabularies.} LOV catalogue and related work on profiling 
LOD can serve as a foundation to build a comprehensive list of most used vocabularies. Such as list
can be updated frequently (e.g., annually) as the LOD cloud statistics is launched to monitor the ranking 
of vocabularies. This will need to combine different efforts and better synchronization and integration of data from different sources (e.g., LODStats, lod-cloud.net and prefix.cc).
\emph{From single to multi-term search.} An area which is still largely unexplored is multi-term vocabulary search. During the ontology design process, it is common to have more than 20 concepts to be represented using existing vocabularies or a new one in case there is no corresponding artifact. While we are able to search for relevant terms in LOV it is still the responsibility of the ontology designer to understand the complex relationships betwen all these terms and come up with a coherent ontology. We could use the network of vocabularies defined in LOV to suggest not only a list of terms but graphs to represent several concepts together.

\emph{Multilingual vocabularies.} There is a need for vocabularies to support more languages. Labels are the main entry point to a vocabulary and their associated language is the key. Only 18\% of LOV vocabularies use a different language than English. Multilingualism is important at least for two reasons: 
\begin{inparaenum}[1)] 
	\item the most obvious one is allowing users to search, query and navigate vocabularies in their native language; and
	\item translating is a process through which the quality of a vocabulary can only improve. Looking at a vocabulary through the eyes of other languages and identifying the difficulties of translation helps to better outline the initial concepts and if necessary refine or revise them. 
\end{inparaenum} 
Hence multilingualism and translation should be native, built-in features of any vocabulary construction, not a marginal task.

\emph{Query extension and rewriting.} Another research perspective is SPARQL query extension and rewriting based on Linked Vocabularies. Using the inter-vocabulary relationships we could transform the query to use the same semantic (same vocabulary terms) as the data source(s) to query.


\section*{Acknowledgments}
This work has been partially supported by the French National Research Agency (ANR) within the Datalift Project, under grant number ANR-10-CORD-009; the Spanish project BabelData (TIN2010-17550) and Fujitsu Laboratories Limited. The Linked Open Vocabularies initiative is graciously hosted by the Open Knowledge Foundation. We would like to thank all the members of LOV community, all the editors and publishers of vocabularies who trust in LOV catalogue. 


\bibliographystyle{plain}
\bibliography{lov}
\end{document}
